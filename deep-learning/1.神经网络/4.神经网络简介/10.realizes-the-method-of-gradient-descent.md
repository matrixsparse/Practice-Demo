# realizes the method of gradient descent

## 如何更新我们的权重

```bash
ΔW​ij=ηδjXi
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3pgsa1w3j20gq016a9u.jpg)

如何实现一次更新，那我们如何把代码转化为能够计算多次权重更新，使得我们的网络能够真正学习呢？
​​
## 案例

```bash
拿一个研究生学院录取数据，用梯度下降训练一个网络

数据有三个输入特征：GRE 分数、GPA 分数和本科院校排名（从 1 到 4）

排名 1 代表最好，排名 4 代表最差
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3pg4j52kj20jj0gyn04.jpg)

### 目标

```bash
基于这些特征来预测一个学生能否被研究生院录取

将使用有一个输出层的网络 , 用 sigmoid 做为激活函数
```

### 数据清理

先做数据转换

rank 是`类别特征`，其中的数字并不表示任何相对的值

排名第 2 并不是排名第 1 的两倍
排名第 3 也不是排名第 2 的 1.5 倍

因此，我们需要用 dummy variables 来对 rank 进行编码

把数据分成 4 个新列，用 0 或 1 表示
排名为 1 的行对应 rank_1 列的值为 1 ，其余三列的值为 0
排名为 2 的行对应 rank_2 列的值为 1 ，其余三列的值为 0

还需要把 GRE 和 GPA 数据标准化，也就是说使得它们的均值为 0，标准偏差为 1 <-- 怎么做？

因为 sigmoid 函数会挤压很大或者很小的输入
很大或者很小输入的梯度为 0，这意味着梯度下降的步长也会是 0

由于 GRE 和 GPA 的值都相当大，我们在初始化权重的时候需要非常小心，否则梯度下降步长将会消失，网络也没法训练了。相对地，如果我们对数据做了标准化处理，就能更容易地对权重进行初始化

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3po7puv5j20j80f5410.jpg)

现在数据已经准备好了，我们看到有六个输入特征：gre、gpa，以及四个 rank 的虚拟变量 （dummy variables）。

### 均方差

这里我们要对如何计算误差做一点小改变。我们不计算 SSE

而是用误差平方的均值（mean of the square errors，MSE）

现在我们要处理很多数据，把所有权重更新加起来会导致很大的更新，使得梯度下降无法收敛

为了避免这种情况，你需要一个很小的学习率

这里我们还可以除以数据点的数量 m 来取平均

这样，无论我们有多少数据，我们的学习率通常会在 0.01 to 0.001 之间

我们用 MSE（下图）来计算梯度，结果跟之前一样，只是取了平均而不是取和

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3pvw89czj20gs03amx3.jpg)

这是用梯度下降来更新权重的算法概述：

* 权重步长设定为0：ΔWi=0
* 对训练数据中的每一条记录：
  * 通过网络做正向传播，计算输出 y^=f(∑iWiXi)
  * 计算输出单元的误差项（error term） δ=(y−​y^)∗f′(∑iWiXi)
  * 更新权重步长 ΔWi=ΔWi+δXi​
* 更新权重 Wi=Wi+ηΔw​i/m 其中 η 是学习率， m 是数据点个数。这里对权重步长做了平均，为的是降低训练数据中大的变化
* 重复 e 代（epoch）

你也可以对每条记录更新权重，而不是把所有记录都训练过之后再取平均。

这里我们还是使用 sigmoid 作为激活函数

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3q0cy3i7j20ja04bwei.jpg)

### 用 NumPy 来实现

首先需要初始化权重

我们希望它们比较小，这样输入在 sigmoid 函数那里可以在接近 0 的位置，而不是最高或者最低处

很重要的一点是要随机地初始化它们，这样它们有不同的初始值，是发散且不对称的

所以我们用一个中心为 0 的正态分布来初始化权重，此正态分布的标准差（scale 参数）最好使用 1/√n
​​其中 n 是输入单元的个数。这样就算是输入单元的数量变多，sigmoid 的输入还能保持比较小

```bash
weights = np.random.normal(scale=1/n_features**.5, size=n_features)
```

NumPy 提供了一个可以让两个数组做点乘的函数，它可以让我们方便地计算 h

`点乘`就是把`两个数组的元素对应位置相乘之后再相加`

```bash
# input to the output layer
# 输出层的输入
output_in = np.dot(weights, inputs)
```

```bash
最后可以用 weights += ... 更新 ΔWi和W​i
​​
weights += ... 是 weights = weights + ... 的简写
```

### 效率提示

因为这里我们用的是 sigmoid 函数，你可以节省一些计算

对于 sigmoid 函数来说，f′(h)=f(h)(1−f(h))

也就是说一旦你有了 f(h)，你就可以直接用它的值来计算误差的梯度了

## 实现梯度下降

```bash
实现梯度下降，用录取数据来训练它

训练一个网络直到你达到训练数据的最小的均方差 mean square error(MSE)
```

* 网络的输出：output
* 输出误差：error
* 误差项：error_term
* 权重步长更新：del_w +=
* 权重更新：weights +=

>make_gradient.py

```bash
#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @Copyright (C), 2018, matrix

import numpy as np
from data_prep import features, targets, features_test, targets_test


# 激活函数
def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1 / (1 + np.exp(-x))


# TODO: We haven't provided the sigmoid_prime function like we did in
#       the previous lesson to encourage you to come up with a more
#       efficient solution. If you need a hint, check out the comments
#       in solution.py from the previous lecture.
def sigmoid_prime(x):
    """
    Calulate f'(h)
    """
    return sigmoid(x) * (1 - sigmoid(x))


# Use to same seed to make debugging easier
np.random.seed(42)

n_records, n_features = features.shape
last_loss = None

# Initialize weights
# 初始化权重，用一个中心为0的正态分布来初始化权重，此正态分布的标准差(scale参数)最好使用
weights = np.random.normal(scale=1 / n_features ** .5, size=n_features)

# Neural Network hyperparameters
epochs = 1000
learnrate = 0.5

for e in range(epochs):
    del_w = np.zeros(weights.shape)
    for x, y in zip(features.values, targets):
        # Loop through all records, x is the input, y is the target

        # Note: We haven't included the h variable from the previous
        #       lesson. You can add it if you want, or you can calculate
        #       the h together with the output

        # TODO: Calculate the output
        # 神经网络的输出
        h = np.dot(x, weights)
        output = sigmoid(h)

        # TODO: Calculate the error
        # 输出误差
        error = y - output

        # TODO: Calculate the error term
        # 输出误差
        output_grad = sigmoid_prime(h)
        error_term = error * output_grad

        # TODO: Calculate the change in weights for this sample
        #       and add it to the total weight change
        # 权重步长更新
        del_w += error_term * x

    # TODO: Update weights using the learning rate and the average change in weights
    # 权重更新
    weights += learnrate * del_w / n_records

    # Printing out the mean square error on the training set
    if e % (epochs / 10) == 0:
        out = sigmoid(np.dot(features, weights))
        loss = np.mean((out - targets) ** 2)
        if last_loss and last_loss < loss:
            print("Train loss: ", loss, "  WARNING - Loss Increasing")
        else:
            print("Train loss: ", loss)
        last_loss = loss

# Calculate accuracy on test data
tes_out = sigmoid(np.dot(features_test, weights))
predictions = tes_out > 0.5
accuracy = np.mean(predictions == targets_test)
print("Prediction accuracy: {:.3f}".format(accuracy))
```
