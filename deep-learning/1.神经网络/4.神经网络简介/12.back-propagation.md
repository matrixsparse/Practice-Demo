# Back propagation【反向传播】

对于这个多层神经网络，我们仍希望使用梯度下降法对其训练，此前我们已经学过，如何计算输出节点的误差项

借助梯度下降法，我们可以使用此误差项来训练隐藏层到输出层的权重

但是训练输入层到隐藏层的权重，我们需要知道隐藏层单元对应的误差项，那么如何求取梯度下降所需的误差项呢？

此前，我们通过对误差平方求关于"输入层到输出层权重"的偏导来计算误差项

若加入隐藏层，使用链式法则时会发现，隐藏层误差项与输出层误差项成正比，比例系数由两层之间的权重决定

隐藏层与输出节点连接越强，则对最终输出值误差项的影响越大

图中可以看到误差项乘以权重值，这与神经网络输入值的正向传播类似即输入值乘以层间权重值，相对于输入值的正向传播，这是误差项的反向传播

由此可以看作，首先反转神经网络，然后将误差项用作输入值，此方法称为反向传播

即使加深层数，这个过程也是一样的，只需逐层不断传播误差项

反向传播是训练神经网络的基础原理，因此对于构建深度学习模型理解反向传播至关重要

如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸

以一个两层神经网络为例，可以使用链式法则计算输入层-隐藏层间权重的误差

要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响

每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播

既然我们知道输出误差，便可以用权重来反向传播到隐藏层

输出层每个输出节点 k 的误差是 δ​ko
​​
隐藏节点 j 的误差即为输出误差乘以输出层-隐藏层间的权重矩阵（以及梯度）

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn49s12wndj218e044jrr.jpg)

梯度下降与之前相同，只是用新的误差：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn49s26lstj217o03mgln.jpg)


其中 w_{ij} 是输入和隐藏层之间的权重， x_i 是输入值。这个形式可以表示任意层数。权重更新步长等于步长乘以层输出误差再乘以该层的输入值

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4a1e3narj217o03iaa4.jpg)

现在，你有了输出误差，\delta_{output}，便可以反向传播这些误差了。V_{in} 是该层的输入，比如经过隐藏层激活函数的输出值

## 范例

以一个简单的两层神经网络为例，计算其权重的更新过程。假设该神经网络包含两个输入值，一个隐藏节点和一个输出节点，隐藏层和输出层的激活函数都是 sigmoid，如下图所示。（注意：图底部的节点为输入值，图顶部的 y​^
​​为输出值。输入层不计入层数，所以该结构被称为两层神经网络。）

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4a2bvgpzj216o0lmtae.jpg)

假设我们试着训练一些二进制数据，目标值是 y=1。我们从正向传播开始，首先计算输入到隐藏层节点

h=∑iwixi=0.1×0.4−0.2×0.3=−0.02
