# The simplest neural network

```bash
目前为止，我们接触的感知器的输出非 0 即 1，输出单元的输入经过了一个激活函数 f(h) 在此处就是指阶跃函数。
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn1y4szbevj215414yq51.jpg)

输出单元返回的是 f(h) 的结果，其中 h 是输出单元的输入：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn1y5xj9qbj217w02imx6.jpg)

下图展示了一个简单的神经网络。权重、输入和偏置项的线性组合构成了输入 h，其通过激活函数 f(h)，给出感知器最终的输出，标记为 y

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn1y5zrbakj21cg0s4n1c.jpg)

这个架构最酷的一点，也是使得神经网络可以实现的原因，就是激活函数 f(h) 可以是 任何函数，并不只是上面提到的阶跃函数。

例如，如果让 f(h)=h，输出等于输入，那网络的输出就是：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn1y8k9993j216u02i0st.jpg)

它跟线性回归模型是一样的！

其它常见激活函数还有对数几率（又称作 sigmoid），tanh 和 softmax。这里主要使用 sigmoid 函数：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn1y8lpx39j219s0vegov.jpg)

sigmoid 函数值域是 0 到 1 之间，它的输出还可以被解释为成功的概率。实际上，用 sigmoid 函数作为激活函数的结果，跟对数几率回归是一样的。

这就是感知器到神经网络的改变，在这个简单的网络中，跟通常的线性模型例如对数几率模型相比，神经网络还没有展现出任何优势

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn1yachbimj21co0wy10q.jpg)

在 XOR 感知器中，虽然把感知器组合起来可以对线性不可分的数据建模，但是却无法对回归模型建模。

一旦开始用连续且可导的激活函数后，就能够运用梯度下降来训练网络

## 使用 NumPy 构建一个简单网络

* 实现 sigmoid 激活函数
* 计算神经网络输出

```bash
构建简单神经网络，它有两个输入节点，一个输出节点，激活函数是 sigmoid
```

sigmoid 函数公式是：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn1y8k9993j216u02i0st.jpg)

指数可以使用 NumPy 的指数函数 np.exp

这个网络的输出为：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn1ye51zquj20vo02qjri.jpg)

要计算加权求和，可以让元素相乘再相加，或者使用 NumPy 的 点乘函数

```bash
#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @Copyright (C), 2018, matrix

import numpy as np


def sigmoid(x):
    # TODO: Implement sigmoid function
    return 1 / (1 + np.exp(-x))


inputs = np.array([0.7, -0.3])
weights = np.array([0.1, 0.8])
bias = -0.1

# TODO: Calculate the output
output = sigmoid(np.dot(inputs, weights) + bias)

print('Output:')
print(output)
```
