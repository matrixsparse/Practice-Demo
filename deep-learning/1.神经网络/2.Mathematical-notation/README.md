# Mathematical notation

数学帮我们定义规则，为神经网络指定规则，让我们可以从数据中学习

## 深度学习主要涉及数学的三个分支：

```bash
线性代数
统计学
微积分
```

## 构建深度学习管道的四个步骤：

```bash
1.collect Data
2.Build Model
3.Train Model
4.Test Model
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn19wocjo2j21ao0xsals.jpg)

清理数据，去掉不必要的特征
真正要用到数学的地方叫做归一化，这是一个可选步骤，可帮助我们的模型，更快达到收敛，即我们的预测值与实际值得误差最小时，能更快地达到那个点，因为所有值都在相同的尺度被运算

归一化数据的策略有多个，一个常用的是"区间缩放法"

如果给定一些数据，可以通过以下公式来归一化它

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn0fg0ylgkj21ma0dyagr.jpg)

取列表中每个值，并从中减去最小值，然后除以最大值减最小值的差，就会获得一个新的数据列表，值都是0到1之间，对每个特征都做如此处理，这样它们都处于相同的区间

归一化数据后，线性代数要确保数据都是神经网络可以接受的形式

## 线性代数4大概念：

```bash
1.标量：单独的数
2.向量：一维数组
3.矩阵：二维数组
4.张量：N维数组
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn0fl5lwlrj225c166k30.jpg)

想将所有类型的数据都转换称为张量，不论图像、文字、视频都变为张量，这里的n就是数据拥有的特征数量，它决定了张量的维数

## 使用三层前馈神经网络

在给定输入的情况下，预测二进制输出

在做归一化时，需要使用数学和深度学习
通过搜索学习模型参数，用随机数来初始化权值
张量流，从输入到输出，计算误差来确定疑问
它给我们实际值和预期值，用反向传播来修正权值

导入唯一要依赖的库

```bash
import numpy as np
x = np.array([[0,0,1],
	[0,1,1],
	[1,0,1],
	[1,1,1]])

y = np.array([[0],
	[1],
	[1],
	[0]])
```

## 建立深度神经网络

深度网络具有所谓的超参数，它们就是我们所定义的网络高级调节按钮，能帮助确定网络架构的一些特征，如模型运行速度多快，每层有多少个神经元，有多少个隐藏层
基本上而言，你的神经网络越复杂，超参数就越多，可以手动调节这些超参，根据自己的知识和对问题的了解
`去猜测可能的值，并观察结果，根据得到的结果，你可以相应地进行调整超参，并重复该过程`
还有一个可用的策略，就是"随机搜索"，可以确定每个超参的取值范围
然后创建一个搜索算法进行随机选值，在指定的取值范围内从均匀分布的概率中取值，也就是说每个值被选中的概率都是一样的，一直重复这一过程，只到找到最佳超参数

这里，我们只有num_epochs作为超参数，因为我们的神经网络非常简单，我们也将使用概率来决定权重值，一个常用的方法，就是从一个低方差的正态分布中，随机初始化各权重样本,这意味着，它们的取值都非常接近

```bash
# build model
num_epochs = 60000

# initialize weights
syn0 = 2*np.random.random((3,4)) =1
syn1 = 2*np.random.random((4,1)) = 1
print(syn0)
```

创建一个3*4的权重矩阵，这是我们输入数据的尺寸，输入层的每个节点，都与下一层的每个节点相连接，权值的范围将是-1到1，由于我们有三个层，所以将初始化两个权重矩阵，下一组权重的尺寸为4*1，也就是输出数据的尺寸，随着数据再神经网络中正向传播，每个层都进行各自相应的运算，以某种方式处理输入值，最终输出一个预测值，这些全部是线性代数、张量数学，

初始化一个for循环，来对网络进行60,000次训练迭代
初始化网络的层，第一层"输入层"获取输入数据
下一层将计算第一层和第一个权重矩阵的点积
当我们将两个矩阵相乘
如对输入数据赋予权重
我们将它称之为"点乘"
然后，对结果进行非线性运算

![点乘](http://ww1.sinaimg.cn/large/dc05ba18gy1fn19wt2v0hj21ao0ki4dh.jpg)

用的是sigmoid函数，它接收一个实际数值，
然后将它"挤"到0和1的范围内，
所以这就是第1层发生的运算，
下一层也是一样的，
我们将第一层的结果，
正向传播至第2层，计算它与下一个权重矩阵的点积
然后用非线性函数，将值转换成输出概率(0至1)范围内

![sigmoid函数](http://ww1.sinaimg.cn/large/dc05ba18gy1fn19wjz5ffj20y60n042m.jpg)

```bash
def nolin(x,deriv=False):
	if (deriv==True):
		return x*(1-x)
	return 1/(1+np.exp(-x))

for j in range(num_epochs):
	# feed forward through layers 0,1 and 2
	10 = X
	11 = nonlin(np.dot(10,syn0))
	12 = nonlin(np.dot(11,syn1))

	# how much did we miss the target value?
	12_error = y - 12

	# in what direction is the target vlaue? 得到加权误差导数，这是一个值矩阵，每个值代表一个预测输出
	12_delta = 12_error*nonlin(12,deriv=True)

	syn1 += 11.T.dot(12_delta)
	syn0 += 10.T.dot(11_delta)
```

由于我们只有三个层，这个输出值就是我们的预测值
我们改进此预测的方式，即网络自我学习的方式，就是逐步优化网络
那么我们如何优化它？

### 如何优化预测

微积分登场，我们模型所做的第一次预测会不准确
所以要改进它，我们首先要量化预测的错误程度
我们通过计算误差，或者说损失
误差表示预测输出值与实际输出值之间的差值有多大
当得到误差值以后，我们希望将它最小化
因为误差越小，预测就越好

训练神经网络的意义就在于随着时间的推移，将误差最小化，
我们不希望更改输入数据，但是我们可以更改权重，来使误差最小化，如果用蛮力强行计算所有，可能的权重，看看哪个会带来最准确的预测值，则会花费，超长的计算时间

相反我们希望获得某种方向感，知道如何更新权重，这在下一轮训练中我们的输出可以更准确，要获得这个方向我们需要计算误差的梯度相对于权值的梯度

计算的方法就是，使用微积分中的导数，当我们将非线性函数的导数设置为True时，它将计算sigmoid函数的导数，即sigmoid函数在给定点的斜率，该点就是我们从第2层得到的预测值，我们想尽可能最小化误差，我们可以直观地将这个过程想象成将一个球，放入碗中，而最小的误差值，就位于碗底，放入球后，我们将计算，每个位置的梯度，如果梯度为负数，则将球右移，如果为正，则将球左移，每次，我们都用梯度相应地更新权值，不断重复这一过程，直到最终梯度变为零，这里就得到了最小误差值，这个过程叫做梯度下降，因为我们将梯度递减到零，并使用它来不断更新我们的权值

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn19wrhg71j22640py49q.jpg)

我们将用预测值计算所得的导数乘以误差，这将得到加权误差导数，我们称它为l2_delta，这是一个值矩阵，每个值代表一个预测输出，它能给我们一个改进的方向，我们可以利用这个方向来更新该层的相关权值，计算指定层的误差，然后用它来计算加权误差梯度，然后朝正确的方向更新权值，这个过程将在每一层循环往复，从最后一层到第一层，我们先用正向传播计算预测值，再将误差向反方向传播，这一过程称为反向传播，
所以我们l2_delta值乘以其相关权重矩阵的转置，来获得上一层的误差，然后用该误差，执行之前相同的运算，获得更新相关层权值的方向值，这样误差就最小化了
最后，我们更新每个相关层的权重矩阵，通过将它们乘以各自的delta，随着时间的推移，误差值在不断下降，我们的预测值最终变得非常准确

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn19wfijfpj212u0nk7ca.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn19vr4916j218q0g2n4a.jpg)

>总结

深度学习借鉴了三个数学分支

```bash
线性代数、统计学、微积分
神经网络对输入张量，进行了一系列操作，来计算预测值
我们可以用梯度下降来优化预测，通过反向传播误差反复循环这一过程，更新每一层的权值
```
