# Word2vec

将单词表示为数字很棒，但是神经网络依然需要学习单词的含义。如果不用一个数字来表示一个单词，我们可否用多个数字来表示单词呢？每个数字都可以用来表示单词的某种抽象特性。如果单词的意思相近，它们的值也会相近。这种表示方法叫做词嵌入。它表示词汇或短语映射在向量上。

创建此类词嵌入的最热门方法之一就是Word2vec。Word2vec 是一种神经网络模型，用文本进行训练，以创建

Word2Vec 存在两种类型：连续词袋 (CBOW) 和 Skip grams。我们将重点介绍 Skip grams，但是涉及的很多内容也适用于 CBOW。Skip grams 模型是一种神经网络，传入一个单词并尝试预测 n 个周围的单词。假设有以下字符串：

```bash
the quick brown fox jumps over the lazy dog
```

该神经网络的输入是"jump"，它将尝试预测 n 个周围的单词，每次预测一个单词。例如，如果 n 为 2，它将尝试预测"fox"和"over"。当该神经网络根据这些单词进行训练时，它将学习这些单词的抽象含义。假设训练后，我们得到以下向量：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnmskq5r0ij216o0g2dhy.jpg)

每个维度都是神经网络从数据集中得出的抽象含义。为了简便起见，我们使用大小为 2 的向量，但是大多数向量的大小都是 25 或更大。"Cat"和"Dog"的单词向量在年龄上的值比"Kitten"和"Puppy"的要大。此数值并不是对年龄的预测，而是对单词之间关系的预测。也就是说，具有相似年龄的单词，它们的值会更相近

## 线性关系

因为相似的单词会出现在相似的上下文中，所以如上所示，单词向量之间形成了线性关系。这一属性让我们可以通过对单词向量进行向量运算，回答一些类比问题。例如：What is an old kitten?

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnmslz8jxoj216w05udgo.jpg)


我们可以按照以下方式回答此问题：将表示“old”的数字与"kitten”的单词向量相加。为了计算出这一数值，我们从"Dog”单词向量中减去与物种“狗”相关的所有内容(家犬)。这可以通过将"Dog"单词向量减去"Puppy”单词向量得到，所得向量如下：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnmsmp2dycj217a05kq34.jpg)

将此向量添加到“kitten”上，会获得最终结果，即“Cat”的单词向量：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnmsmzmig8j218205mweo.jpg)

一切看起来很棒，但这还没有结束。这是单词的一种表现形式，你会把它提供给神经网络，用于解决更复杂的问题。在下个部分，你将学习 NLP 问题常用的模型，即长短期记忆网络(LSTM)。
