# 分类交叉熵

使用平方误差的和作为网络的成本函数，但是当时我们只有单个（标量）输出值

但是当你在使用 softmax 时，输出是向量。一个`向量`是`输出单元的概率值`。你还可以使用一种叫`独热编码(one-hot encoding)`的方法，用`向量`表示`数据标签`

这只是表示你有一个长度为类别数量的向量，标签元素标记为 1，而其他标签设为 0。对于之前的数字分类示例，图片数字 4 的标签向量是

```bash
y=[0,0,0,0,1,0,0,0,0,0]
```

输出预测向量为：

```bash
​y^=[0.047,0.048,0.061,0.07,0.330,0.062,0.001,0.213,0.013,0.150]
```

我们希望误差与这些向量之间的距离成比例。要计算这一距离，我们将使用交叉熵。我们的神经网络训练目标将为：`通过尽可能地减小交叉熵使预测向量与标签向量尽量靠近`

>交叉熵计算公式

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnhqst5tmyj21960ja0wr.jpg)

可以从上文中看出，`交叉熵`等于`标签元素的和`乘以`预测概率的自然对数`。注意，该等式并不对称！千万不能交换向量，因为标签向量里有很多 0，对 0 求对数将产生错误

对标签向量使用独热编码的好处是除了为真的标签数值是1之外，其他所有的 yj
项都为 0 。因此，除了y​j=1，其他所有项加起来为 0，交叉熵直接变成D=−ln
​y^ 。例如，输入图片为数字 4 并且标为 4，那么只有与 4 对应的单元的输出，在交叉熵成本函数中才会产生影响
