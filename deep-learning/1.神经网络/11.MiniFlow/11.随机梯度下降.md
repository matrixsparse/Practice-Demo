# 随机梯度下降

```bash
class Sigmoid(Node):
    """
    Represents a node that performs the sigmoid activation function.
    """
    def __init__(self, node):
        # The base class constructor.
        Node.__init__(self, [node])

    def _sigmoid(self, x):
        """
        This method is separate from `forward` because it
        will be used with `backward` as well.

        `x`: A numpy array-like object.
        """
        return 1. / (1. + np.exp(-x))

    def forward(self):
        """
        Perform the sigmoid function and set the value.
        """
        input_value = self.inbound_nodes[0].value
        self.value = self._sigmoid(input_value)

    def backward(self):
        """
        Calculates the gradient using the derivative of
        the sigmoid function.
        """
        # Initialize the gradients to 0.
        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}
        # Sum the derivative with respect to the input over all the outputs.
        for n in self.outbound_nodes:
            grad_cost = n.gradients[self]
            sigmoid = self.value
            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnt02g2297j20mv06cmxn.jpg)

## 随机梯度下降

随机梯度下降 (SGD) 是一种梯度下降形式，其中对于每次前向传递，都会从总的数据集中随机选择一批数据。还记得之前讨论的批量大小吗？即批次大小

理想情况下，每次前向传递时，都会将整个数据集提供给神经网络

但是实际操作中，这么做是不现实的，因为内存有限

随机梯度下降是梯度下降的近视值，神经网络处理的批次越多，近视值就越准确。 SGD 的实现包括：

从总的数据集中随机抽样一批数据
前向和后向运行网络，计算梯度（根据第 (1) 步的数据）
应用梯度下降更新。
重复第 1-3 步，直到出现收敛情况或者循环被其他机制暂停（即迭代次数）
如果一切顺利，网络的损失应该通常会下降，表示权重和偏置越来越有用

到目前为止，MiniFlow 已经可以完成第 2 步。在下面的测验中，第 1 和 4 步已执行。你需要执行第 3 步。

提醒下，下面是梯度下降更新方程，其中 \alphaα 表示学习速度：

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnt03gbv3zj20jt01ddfm.jpg)

>这道练习，我们将使用实际的数据集——波士顿房价数据集。经过训练后，该网络将能够预测波士顿的房价！

数据集中的每个示例都是对波士顿郊区住房的描述，描述内容包括 13 个数字值（特征）

每个示例还具有相关的价格

我们将通过 SGD 尽量减小实际房价与神经网络根据这些特征预测的房价之间的均方误差

如果一切顺利，输出应该如下所示：

当批次大小为 11 时：

```bash
Total number of examples = 506
Epoch: 1, Loss: 140.256
Epoch: 2, Loss: 34.570
Epoch: 3, Loss: 27.501
Epoch: 4, Loss: 25.343
Epoch: 5, Loss: 20.421
Epoch: 6, Loss: 17.600
Epoch: 7, Loss: 18.176
Epoch: 8, Loss: 16.812
Epoch: 9, Loss: 15.531
Epoch: 10, Loss: 16.429
```

当批次大小和示例总数相等时（批量为整个数据集）：

```bash
Total number of examples = 506
Epoch: 1, Loss: 646.134
Epoch: 2, Loss: 587.867
Epoch: 3, Loss: 510.707
Epoch: 4, Loss: 446.558
Epoch: 5, Loss: 407.695
Epoch: 6, Loss: 324.440
Epoch: 7, Loss: 295.542
Epoch: 8, Loss: 251.599
Epoch: 9, Loss: 219.888
Epoch: 10, Loss: 216.155
```

注意代价或损失接近 0
