# 卷积层

将完全连接层替换为局部连接层这一概念，这种局部连接层包含更少的权重并在空间内共享，并定义了卷积层，它是一种隐藏层

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fofye6ms03j21lg0vqb1t.jpg)

首先将图片拆分为更小的部分，将图片分成四个相同大小的区域，标为红色、绿色、黄色和蓝色

在拆分图片以构建卷积层的过程中，首先选择定义卷积窗的宽度和高度，然后在图片像素矩阵上，水平和垂直地滑动该卷积窗

在每个位置，卷积窗都在图片中指定一个小部分，并定义一组连接到单个隐藏节点的像素，我们将此隐藏层称为卷积层

>我们深入了解下区域性输入节点集合，对卷积层中的节点的值有何影响

为了获取该图片输入的卷积层中节点的值，我们就像处理神经网络一样操作

我们将输入节点与对应的权重相乘，然后对结果求和，求和后的结果是0

和神经网络一样存在偏差，暂时假设该偏差是0，我们始终会卷积层添加一个ReLu激活函数，ReLu激活函数使正值保持不变并将所有负值变成0

这里0保持不变，因此我可以代入卷积层第一个节点的值，然后按照完全相同的方式计算所有其他节点的值

首先将每个输入节点与权重相乘，这个节点的值是-2，偏差依然是0，所以不用加上任何值，但是现在应用ReLu激活函数的话-2变成了0，算出卷积层所有其他节点的值，我们会发现将权重放在网格里，比写在剪头上方更方便，以这种方式将权重放入网络中后，我们将该网络成为过滤器，它的大小始终与卷积窗的大小一样，这里我们的过滤器宽和高都为3，现在计算隐藏层中节点的值这一流程，我们看看计算卷积层中第一个节点的值效果如何，同样我们首先将每个输入节点，与对应的权重相乘，然后对结果求和，求和之后是0，因此应用ReLu激活函数后还是0，但是对于这个大的值3会怎样？你会发现过滤器中的正值，正好对应的是这个图片区域最大的值，反之亦然，过滤器中的负值对应的是图片中最小的值，实际上，因为我们对图片中的像素进行了调整，使其位于0到1之间，1表示白色，0表示黑色，这个3是我们可以从此过滤器的卷积层中获得的最大值，这也是该区域的规律，对角线白色长条，是唯一可以产生这个最大值的像素分布，我们看到这里也出现了3，并且可以验证图片的相应区域是一样的，但是再看看过滤器，我们用浅色表示更大的数字，可以看出规律是相符的，每个都描绘了一个浅色斜条纹，该可视化图表会告诉我们，过滤器会检测到什么样的规律，就像在神经网络中一样，我们不会提前设置这些权重，而是由网络去判断哪些权重可以最小化损失函数，到目前为止，我们网络只有一个过滤器，因此我们只能获得是否检测到了一个规律的信息，如果我们想检测更多规律，则需要使用更多过滤器
