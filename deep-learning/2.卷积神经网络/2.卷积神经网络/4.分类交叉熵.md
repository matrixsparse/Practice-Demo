# 分类交叉熵

目前所有约600,000个权重都具有随机性，因此模型是随机预测结果

通过训练模型，我们将修改这些权重并改善预测结果

但是在训练模型之前，我们需要指定损失函数，因此我们构建的是多类别分类器

因此将使用分类交叉熵损失，该损失函数通过将模型的`预测结果与实际标签进行对比`

看看我们的模型是否能够很好地分类图片

注意：真正的标签是一位热码编码

每个标签是具有10个项目的向量，模型输出向量也是10个项目

假设模型返回这个预测结果，它预测图片中的数字是8的概率是90%，是3的概率是10%

实际上，你还可以将标签向量看做概率，它知道图片有100%的概率描绘的是3

分类交叉熵损失查看这两个向量，如果这两个向量对图片中的数字保持相同的意见，那么返回一个较低的值

这里，模型非常确定数字是8，但是标签很确定数字是3

因此损失函数将返回一个更高的值

总结下，我们看到如果模型预测结果与标签的一致，那么损失很低，我们对好的模型也是这种期望

我们希望预测结果与标签的一致

我们将尝试寻找使预测结果能够最小化损失函数的模型参数

之前，将损失函数想象成代表山峦的表面，要最小化函数

我们只需找到降落到最低山谷的道路

降低损失函数的标准方法是梯度下降

我们介绍了几种实现梯度下降的方法

在Keras中每种方法都对应一个优化程序

这里描绘的表面是一种损失函数示例

所有优化程序都希望获得函数最小值，可以看出有些优化程序的效果比其他的更好

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fntqg11up2j21ae0u07wh.jpg)

当编译这个模型的时候，我们将指定损失函数和优化程序

```bash
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
```

通过将这个额外参数添加为准确率指标

我们将能够检查我们的模型准确率在训练流程中是如何变化的

编译模型后，可以先查看对于测试集，它已经具备的准确率，然后再去训练它，我们并不期望它比随机情况效果更好，这里对应的是10%的准确率

```bash
# evaludate test accuracy
score = model.evaluate(X_test,y_test,verbose=0)
accuracy = 100*score[1]

# print test accuracy
print('Test accuracy：%.4f%%' % accuracy)
```
