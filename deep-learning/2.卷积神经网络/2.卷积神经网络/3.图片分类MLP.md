# 图片分类 MLP

我们将创建一个神经网络，用于发现数据中的规律，训练之后，我们将能够使用该网络，分类新图片中包含的数字

我们数据点是由784个项目的向量，输入层将有784个节点，我们先从两个隐藏节点开始，每个均包含512个节点，我们的输出层需要在10个不同的数字之间进行区分，因此节点设为10个

我始终会在最终完全连接层上，添加一个softmax激活函数，这样可以确保网络会输出

图片描绘的是每个潜在数字的概率估值

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fntpnr72bij21nq0se1kx.jpg)

我们将在keras中指定这个草稿模型，如果你还记得如何指定神经网络的，那么这段代码看起来应该很相似，我只增加一层，即展平层

```bash
#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @Copyright (C), 2018, matrix

from keras.datasets import mnist

from keras.models import Sequential
from keras.layers import Dense, Flatten

# 1.Load MNIST DataBase
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# define the model
model = Sequential()
model.add(Flatten(input_shape=X_train.shape[1:]))
model.add(Dense(512))
model.add(Dense(512))
model.add(Dense(10, activation='softmax'))

# summarize the model
model.summary()
```

这一层在指定MLP之前，它获得图片矩阵的输入并将其转换为向量，在模型摘要中可以看到它输出了一个具有784个项目的向量

```bash
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               401920    
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dense_3 (Dense)              (None, 10)                5130      
=================================================================
Total params: 669,706
Trainable params: 669,706
Non-trainable params: 0
_________________________________________________________________
```

对于首次创建而言，这个模型不错了

我们将向所有隐藏层添加一个ReLU激活函数，注意这个函数不会处理所有正值，并将所有负值处理为0

我们知道ReLI函数可以帮助解决梯度消失问题

通过添加ReLU函数，可以使模型的准确率大大提高

该激活函数也广泛应用于卷积神经网络，训练该新模型后，你会发现某些过拟合现象

模型能够很好地预测训练数据集中的数字，但是测试图片的识别效果却不太好

在此期间，为了尽量避免过拟合现象，可以添加dropout层，我们将向该模型添加几个dropout层

## 注意

必须向dropout层提供0到1之间的参数，该值对应的是网络中的任何节点，在训练期间被忽略的概率

在设定该值时建议先从小的值开始，看看网络有何响应，然后如果觉得有必要增大的话，再增大这一值

>代码实现

```bash
#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @Copyright (C), 2018, matrix

from keras.datasets import mnist

from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten

# 1.Load MNIST DataBase
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# define the model
model = Sequential()
model.add(Flatten(input_shape=X_train.shape[1:]))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))

# summarize the model
model.summary()
```

>运行结果

```bash
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               401920    
_________________________________________________________________
dropout_1 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                5130      
=================================================================
Total params: 669,706
Trainable params: 669,706
Non-trainable params: 0
_________________________________________________________________
```

和神经网络一样，我们通过首先创建一个序列模型来创建一个 CNN。

```bash
from keras.models import Sequential
```

```bash
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
```

和神经网络一样，通过使用 .add() 方法向网络中添加层级：

```bash
model = Sequential()
model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))
model.add(MaxPooling2D(pool_size=2))
model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dense(10, activation='softmax'))
```

该网络以三个卷积层（后面跟着最大池化层）序列开始。前 6 个层级旨在将图片像素数组输入转换为所有空间休息都丢失、仅保留图片内容信息的数组 。然后在 CNN 的第七个层级将该数组扁平化为向量。后面跟着两个密集层，旨在进一步说明图片中的内容。最后一层针对数据集中的每个对象类别都有一个条目，并具有一个 softmax 激活函数，使其返回概率

注意：在该视频中，你可能注意到卷积层表示为 Convolution2D，而不是 Conv2D。对于 Keras 2.0 来说，二者都可以，但是最好使用 Conv2D

## 注意事项

始终向 CNN 中的 Conv2D 层添加 ReLU 激活函数。但是网络的最后层级除外，密集层也应该具有 ReLU 激活函数
在构建分类网络时，网络中的最后层级应该是具有 softmax 激活函数的 密集层。最后层级的节点数量应该等于数据集中的类别总数
建议参阅 Andrej Karpathy 的 tumblr，其中包含了用户提交的损失函数，对应的是本身有问题的模型。损失函数在训练期间应该是减小的，但是这些图表显示的却是非常不同的行为 :)
