# Perceptron【感知器】

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4eqx3xgpj20nd0ah0tv.jpg)

一个简单的神经网络如何做决策：输入数据，处理信息，然后给出一个结果作为输出！

数据，无论是考试成绩还是评级，被输入到一个相互连接的节点网络中。

这些独立的节点被称作感知器 或者神经元。

它们是构成神经网络的`基本单元`

每个感知器依照输入数据来决定`如何对数据分类`

在上面的例子中，输入的评级或者成绩要么通过阈值 (threshold) 要么通不过

这些分类组合形成一个决策 - 例如，如果两个节点都返回 “yes“，这个学生就被学校录取了

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4es7olxej20mb09ymyy.jpg)

让我们进一步放大来看一个单个感知器如何处理输入数据。

上图中的感知器是视频中决定学生是否被录取的两个感知器之一

它决定了学生的评级是否应该被学校录取。

你也许会问：“它怎么知道在做录取决定的时候是分数更重要还是评级更重要？”

事实上，当我们初始化神经网络的时候，不知道哪个信息对决定更重要

这需要神经网络 自己学习 出哪个数据更重要，然后调整。

神经网络通过一个叫做 Weight（权重）的东西来做这件事

## 权重

当数据被输入感知器，它会与分配给这个特定输入的权重相乘。例如，上图感知器有两个输入，tests和 grades，所以它有两个与之相关的权重，并且可以分别调整。这些权重刚开始是随机值，当神经网络学习到什么样的输入数据会使得学生被学校录取之后，网络会根据之前权重下分类的错误来调整权重，这个过程被称为神经网络的训练。

一个较大的权重意味着神经网络认为这个输入比其它输入更重要，较小的权重意味着数据不是那么重要。一个极端的例子是，如果 test 成绩对学生录取没有影响，那么 test 分数的权重就会是零，也就是说，它对感知器的输出没有影响。

## 输入数据加总

每个感知器的输入需要有一个关联的权重代表它的重要性，这个权重是由神经网络的学习过程决定的，也就是训练。接下来，经过加权的输入数据被加总，生成一个单独的值，它会帮助实现最终输出 - 也就是这个学生是否被录取。让我们看一个实际的例子：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4et7dcbkj20l80c30u0.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4etvwpb7j20p30f1dht.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4eviof66j20oi0723z6.jpg)

## 计算激活函数的输出

最后，感知器求和的结果会被转换成输出信号，这是通过把线性组合传给 激活函数 来实现的

当输入给到节点，`激活函数可以决定节点的输出`

因为它决定了实际输出，我们也把层的输出，称作"激活"

最简单的激活函数之一是单位阶跃函数（Heaviside step function）

如果线性组合小于 0，函数返回 0，如果线性组合等于或者大于 0，函数返回 1

单位阶跃函数（Heaviside step function） 如下图，其中 h 是已计算的线性组合：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4euflkafj20m70kygm0.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4euuizadj20nc02974h.jpg)

用二维数据举例看起来最容易。在下图中，想象线上以及阴影部分的任何一点，代表所有可能对节点的输入。同时，想象 y 轴上的值为输入与适当权重的线性组合结果。并将这个结果作为激活函数的输入。

记得我们说过，单位阶跃函数对任何大于等于 0 的输入，都返回 1 像你在图中看到的，只有一个点的 y 值大于等于 0： 就是 (0,0)原点：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4ewdhiotj20kh0gpdg7.jpg)

当然，我们想要更多可能的 grade/test 组合在录取组里，所以我们需要对我们的激活函数做调整使得它对更多的输入返回 1，特别是，我们要找到一种办法，让所有我们希望录取的人输入和权重的线性组合的值大于等于 0

使我们函数返回更多 1 的一种方式是往我们线性组合的结果里加上一个 偏置项（bias）

偏置项在公式中用 b 来表示，让我们移动一下各个方向上的值。

例如，下图蓝色部分代表先前的假设函数加了 +3 的偏置项。蓝色阴影部分表示所有激活的值。注意，这个结果的输入，跟之前灰色部分的输入是一样的，只是加了偏置项之后，它被调整的更高了

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4ewrb0rej20k00gkq3s.jpg)

现在，我们并不能事先知道神经网络该如何选择偏置项。但没关系，偏置项跟权重一样，可以在训练神经网络的时候更新和改变。增加了偏置项之后，我们有了一个完整的感知器公式：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4ex4pzrvj20ns04ddgb.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn4exb8gskj20nu05djs5.jpg)
