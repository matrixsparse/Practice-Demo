# Gradient descent practice【梯度下降】

## 创建一个新的环境

```bash
[root@sparsematrix ~]# conda create -n neural-network python=2.7.9
```

## 进入环境

```bash
source activate neural-network
```

```bash
conda install numpy pandas
```

之前我们看到一个权重的更新可以这样计算

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3jtxsevkj20kd010we9.jpg)

这里 error term δ 是指

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3o6nryeqj20gr012744.jpg)

>gradient.py

```bash
#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @Copyright (C), 2018, matrix

import numpy as np


# 权重的更新可以这样计算
# ΔWi=ηδXi
# 这里 error term δ 是指
# δ=(y−y^)f′(h)=(y−y^)f′(∑WiXi)
# (y−y^) 是输出误差
# f′(h) 是激活函数 f(h) 的导函数 , 我们把这个导函数称做输出的梯度
# 现在假设只有一个输出单元，用 sigmoid 来作为激活函数 f(h)

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1 / (1 + np.exp(-x))


def sigmoid_prime(x):
    """
    # Derivative of the sigmoid function
    """
    return sigmoid(x) * (1 - sigmoid(x))


# The learning rate , eta in the weight step equation
# 权重更新的学习率
learnrate = 0.5
# Input data
# 输入数据
x = np.array([1, 2, 3, 4])
# Target
# 目标
y = np.array(0.5)

# Initial weights
w = np.array([0.5, -0.5, 0.3, 0.1])

### Calculate one gradient descent step for each weight
### Note: Some steps have been consilated, so there are
###       fewer variable names than in the above sample code

# TODO: Calculate the node's linear combination of inputs and weights
# 输入和权重的线性组合
h = np.dot(x, w)

# TODO: Calculate output of neural network
# 神经网络输出
nn_output = sigmoid(h)

# TODO: Calculate error of neural network
# 输出误差
error = y - nn_output

# TODO: Calculate the error term
#       Remember, this requires the output gradient, which we haven't
#       specifically added a variable for.
# 输出梯度
output_grad = sigmoid_prime(h)

# TODO: Calculate the error term
#       Remember, this requires the output gradient, which we haven't
#       specifically added a variable for.
# error_term = error * sigmoid_prime(h)
# Note: The sigmoid_prime function calculates sigmoid(h) twice,
#       but you've already calculated it once. You can make this
#       code more efficient by calculating the derivative directly
#       rather than calling sigmoid_prime, like this:
# error_term = error * nn_output * (1 - nn_output)
error_term = error * output_grad

# TODO: Calculate change in weights
# Gradient descent step
# 梯度下降一步
del_w = learnrate * error_term * x

print('Neural Network output:')
print(nn_output)
print('Amount of Error:')
print(error)
print('Change in Weights:')
print(del_w)
```

>运行结果

```bash
Neural Network output:
0.689974481128
Amount of Error:
-0.189974481128
Change in Weights:
[-0.02031869 -0.04063738 -0.06095608 -0.08127477]
```
