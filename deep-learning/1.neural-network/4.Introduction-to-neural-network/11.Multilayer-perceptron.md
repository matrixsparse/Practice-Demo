# Multilayer perceptron【多层感知器】

此前XOR感知器例子显示，通过加深层数，模型得以解决非线性问题

这是一个多层感知器的示例，内含3个输入单元，1个输出单元，与2个中间层单元

这个中间层又称为隐藏层

神经网络整体计算方法并未变化，只是现在

隐藏层的激活值被用作输出层的输入值

`隐藏层的输入计算方法等于权重值乘以输入值，加上偏执项`

而且需要使用 sigmoid 等激活函数来计算隐藏层的输出值

隐藏层的激活值通过乘以第二组权重传递到输出层并再次使用激活函数求得最终输出值

通过更深层次的堆叠

神经网络能够学习更加复杂的模式，这是深度学习的名称由来，其强大之处也源于`隐藏层的深度堆叠`

## 实现隐藏层

之前研究的是只有一个输出节点网络，代码也很直观

但是现在我们有多个输入单元和多个隐藏单元

它们的权重需要有两个索引 W​ij，其中 i 表示输入单元，j 表示隐藏单元

例如在下面这个网络图中，输入单元被标注为 x​1,x​2, x3，隐藏层节点是 h1和 h2
​​
![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wkc1bfpj21ig0ui0z7.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wudl3kyj21e80g6q51.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wlwlnmaj219y0sa0zn.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wmd7j3xj215u0j4n22.jpg)

对比上面的示意图，确保你了解了不同的权重在矩阵中与在神经网络中的对应关系。

用 NumPy 来初始化这些权重，我们需要提供矩阵的形状（shape），如果特征是包一个包含以下数据的二维数组：

```bash
# Number of records and input units
# 数据点数量以及每个数据点有多少输入节点
n_records, n_inputs = features.shape
# Number of hidden units
# 隐藏层节点个数
n_hidden = 2
weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))
```

这样创建了一个名为 weights_input_to_hidden 的二维数组，维度是 n_inputs 乘 n_hidden

输入到隐藏节点是所有输入乘以隐藏节点权重的和。所以对每一个隐藏层节点 h​j，我们需要计算：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wnlmfimj217q05q0t6.jpg)


为了实现这点，我们需要运用矩阵乘法，如果你对线性代数的知识有些模糊，我们建议你看下之前先修部分的资料。这里你只需要了解矩阵与向量如何相乘。

在这里，我们把输入（一个行向量）与权重相乘。要实现这个，要把输入点乘（内积）以权重矩阵的每一列。例如要计算到第一个隐藏节点 j=1 的输入，你需要把这个输入与权重矩阵的第一列做点乘：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wo7zhqrj21720yogrt.jpg)

针对第二个隐藏节点的输入，你需要计算输入与第二列的点积，以此类推

在 NumPy 中，你可以直接使用 np.dot

```bash
hidden_inputs = np.dot(inputs, weights_input_to_hidden)
```

你可以定义你的权重矩阵的维度是 n_hidden 乘 n_inputs 然后与列向量形式的输入相乘：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3woshommj218s0bgabc.jpg)

注意：

这里权重的索引在上图中做了改变，与之前图片并不匹配

这是因为，在矩阵标注时行索引永远在列索引之前，所以用之前的方法做标识会引起误导

你只需要了解这跟之前的权重矩阵是一样的，只是做了转换，之前的第一列现在是第一行，之前的第二列现在是第二行

如果用之前的标记，权重矩阵是下面这个样子的：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wpperjvj21ck09kq3u.jpg)

切记，上面标注方式是不正确的，这里只是为了让你更清楚这个矩阵如何跟之前神经网络的权重匹配。

矩阵相乘最重要的是他们的维度相匹配。因为它们在点乘时需要有相同数量的元素

在第一个例子中，输入向量有三列，权重矩阵有三行；第二个例子中，权重矩阵有三列，输入向量有三行

如果维度不匹配，你会得到：

```bash
# Same weights and features as above, but swapped the order
hidden_inputs = np.dot(weights_input_to_hidden, features)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-11-1bfa0f615c45> in <module>()
----> 1 hidden_in = np.dot(weights_input_to_hidden, X)

ValueError: shapes (3,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)
```

3x2 的矩阵跟 3 个元素的数组是没法相乘的。因为矩阵中的两列与数组中的元素个数并不匹配。能够相乘的矩阵如下：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wr2pvc0j21am0ko77y.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fn3wr1r4v8j219m0po42t.jpg)

这里的规则是，如果是数组在左边，数组的元素个数必须与右边矩阵的行数一样。如果矩阵在左边，那么矩阵的列数，需要与右边向量的行数匹配。

## 构建一个列向量

看上面的介绍，你有时会需要一个列向量，尽管 NumPy 默认是行向量。你可以用 arr.T 来对数组进行转置，但对一维数组来说，转置还是行向量

所以你可以用 arr[:,None] 来创建一个列向量：

```bash
print(features)
> array([ 0.49671415, -0.1382643 ,  0.64768854])

print(features.T)
> array([ 0.49671415, -0.1382643 ,  0.64768854])

print(features[:, None])
> array([[ 0.49671415],
       [-0.1382643 ],
       [ 0.64768854]])
```

当然，你可以创建一个二维数组，然后用 arr.T 得到列向量。

```bash
np.array(features, ndmin=2)
> array([[ 0.49671415, -0.1382643 ,  0.64768854]])

np.array(features, ndmin=2).T
> array([[ 0.49671415],
       [-0.1382643 ],
       [ 0.64768854]])
```

我个人更倾向于保持所有向量为一维数组，这样可以更好认知

## 编程练习

下面你要实现一个 4x3x2 网络的正向传播，用 sigmoid 作为两层的激活函数。

* 计算隐藏层的输入
* 计算隐藏层输出
* 计算输出层的输入
* 计算神经网络的输出

>multilayer.py

```bash
#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @Copyright (C), 2018, matrix

import numpy as np


def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1 / (1 + np.exp(-x))


# Network size
N_input = 4
N_hidden = 3
N_output = 2

np.random.seed(42)
# Make some fake data
X = np.random.randn(4)

weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))
weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))

# TODO: Make a forward pass through the network

hidden_layer_in = np.dot(X, weights_input_to_hidden)
hidden_layer_out = sigmoid(hidden_layer_in)

print('Hidden-layer Output:')
print(hidden_layer_out)

output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)
output_layer_out = sigmoid(output_layer_in)

print('Output-layer Output:')
print(output_layer_out)
```

>运行结果

```bash
Hidden-layer Output:
[ 0.41492192  0.42604313  0.5002434 ]
Output-layer Output:
[ 0.49815196  0.48539772]
```
