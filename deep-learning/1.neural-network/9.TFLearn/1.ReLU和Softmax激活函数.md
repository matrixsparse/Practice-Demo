# ReLU 和 Softmax 激活函数

## 激活函数

之前使用 S 型函数作为隐藏单元上的激活函数，对于分类来说，则是输出单元上的激活函数。S 型函数并非是唯一可以使用的激活函数，实际上它具有一些不足

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnhqhrekejj21300i0go6.jpg)

正如在反向传播资料中提到的，S 型函数的导数最大值为 0.25（如上所示）。这意味着，当你用 S 型函数单元进行反向传播时，网络上每层出现的错误至少减少 75%，如果有很多层，权重更新将很小，这些权重需要很长的训练时间。因此，S 型函数不适合作为隐藏单元上的激活函数。

## 初识修正线性单元（ReLU）

近期的大多数深度学习网络都对隐藏层使用修正线性单元 (ReLU)，而不是 S 型函数。如果输入小于 0，修正线性单元的输出是 0，原始输出则相反。即如果输入大于 0，则输出等于输入。从数学角度来看，如下所示：

```bash
f(x)=max(x,0)
```

函数的输出是输入值 x 或 0（取较大者）。如果 x=−1，那么 f(x)=0；如果 x=0.5，那么 f(x)=0.5