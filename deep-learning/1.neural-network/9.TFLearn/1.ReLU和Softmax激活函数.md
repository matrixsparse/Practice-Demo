# ReLU 和 Softmax 激活函数

## 激活函数

之前使用 S 型函数作为隐藏单元上的激活函数，对于分类来说，则是输出单元上的激活函数。S 型函数并非是唯一可以使用的激活函数，实际上它具有一些不足

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnhqhrekejj21300i0go6.jpg)

正如在反向传播资料中提到的，S 型函数的导数最大值为 0.25（如上所示）。这意味着，当你用 S 型函数单元进行反向传播时，网络上每层出现的错误至少减少 75%，如果有很多层，权重更新将很小，这些权重需要很长的训练时间。因此，S 型函数不适合作为隐藏单元上的激活函数。

## 初识修正线性单元（ReLU）

近期的大多数深度学习网络都对隐藏层使用修正线性单元 (ReLU)，而不是 S 型函数。如果输入小于 0，修正线性单元的输出是 0，原始输出则相反。即如果输入大于 0，则输出等于输入。从数学角度来看，如下所示：

```bash
f(x)=max(x,0)
```

函数的输出是输入值 x 或 0（取较大者）。如果 x=−1，那么 f(x)=0；如果 x=0.5，那么 f(x)=0.5

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnhqm12enzj21460v0taf.jpg)

ReLU 激活函数是你可以使用的最简单非线性激活函数。当输入是正数时，导数是 1，所以没有 S 型函数的反向传播错误导致的消失效果。研究表明，对于大型神经网络来说，ReLU 的训练速度要快很多。TensorFlow 和 TFLearn 等大部分框架使你能够轻松地在隐藏层使用 ReLU，你不需要自己去实现这些 ReLU。

### 不足

有时候一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，会使ReLU神经元始终为 0（这个神经元再也不会对任何数据有激活现象了）。这些“无效”的神经元将始终为 0，很多计算在训练中被浪费了。

```bash
遗憾的是，ReLU 单元在训练期间可能会很脆弱并且会变得“无效”。例如，流经 ReLU 神经元的大型梯度可能会导致权重按以下方式更新：神经元将再也不会在任何数据点上激活。如果发生这种情况，那么流经该单元的梯度将自此始终为零。也就是说，ReLU 单元会在训练期间变得无效并且不可逆转，因为它们可能会不再位于数据流形上。例如，学习速度（learning rate）设置的太高，你的网络可能有高达 40% 的神经元处于“无效”状态（即神经元在整个训练数据集上从未激活）。如果能正确地设置学习速度，那么该问题就不太容易出现。
```

### Softmax

之前，我们看到神经网络用在了回归（骑车用户问题）和二元分类（研究生招生问题）中。通常，你会希望预测某个输入是否处于很多类别中的某一个类别。这是一种分类问题，但是 S 型函数不再是最佳选择。我们将会使用 softmax 函数。和 sigmoid 一样，softmax 函数将每个单元的输出压缩到 0 和 1 之间。但 softmax 函数在拆分输出时，会使输出之和等于 1。softmax 函数的输出等于分类概率分布，显示了任何类别为真的概率。

softmax 函数与普通 sigmoid 之间的真正差别是 softmax 会标准化输出，使输出之和等于 1。对于这两种函数，你都可以输入向量，并获得输出为相同大小的向量，但是所有值都压缩在 0 和 1 之间。sigmoid 可用于只有一个输出单元的二元分类。但是如果进行多项分类的话，则需要使用多个输出单元（每个类别一个单元），并对输出进行 softmax 激活。

例如，softmax 函数有三个输入，那么网络有三个输出单元，则结果如下：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnhqpdc8mzj20ue08iwfe.jpg)

softmax 函数的数学表达式如下所示，其中 z 是输出层的输入向量（如果你有 10 个输出单元，则 z 中有 10 个元素）。同样，j 表示输出单元的索引。

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnhqpwc7h7j20q60523yt.jpg)

看上去的确很难懂，但实际上很简单，如果不懂数学原理的话，也没关系。只需记住输出经过压缩，和为 1。

为了更好地理解，可以将训练神经网络看成从图片中识别和分类手写数字。该网络将会有 10 个输出单元，0 到 9 之间的每个数字一个单元。如果你为其提供一个表示数字 4 的图片（如下所示），那么与数字 4 对应的输出单元将会被激活。

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnhqqiksaej20j60im3yx.jpg)

构建这样的网络需要 10 个输出单元，每个数字一个单元。每个训练图片都被为真的数字标记，该网络的目标是预测正确的标签。所以，如果图片的输入为 4，则 4 对应的输出单元将被激活，以此类推。

对于上述示例图片，softmax 函数的输出可能如下所示：

![All text](http://ww1.sinaimg.cn/large/dc05ba18gy1fnhqr3mei0j20vg0i075g.jpg)

该图片很像 4，所以 4 的概率最高，但是看起来也像 7，还有点像 9，只是没有完整的圆形。所以 4 的概率最大，但是 7 和 9 也存在一定的概率。

softmax 可以用于任何数量的分类。接下来你将看到，它可以用于预测两种类别的情感（积极和消极）。还可以用于成百上千的物体分类，例如物体识别问题中，需要识别数以百计不同种类的物体