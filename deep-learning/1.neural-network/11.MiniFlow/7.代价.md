# 代价

你可能觉得奇怪，为何 sigmoid 具有单独的方法。正如在 S 型函数（等式 (4)）的导数中看到的，S 型函数实际上是它自己的导数的一部分

将 sigmoid 分离出来意味着你不需要为前向传播和反向传播实现两次

这很不错！此时，你已经使用了权重和偏置来计算输出。并且你使用了激活函数来对输出进行分类

你可能还记得，神经网络通过修改权重和偏置（根据标签化的数据集进行训练）改善输出的精确度

我们可以采用多种技巧来定义神经网络的精确度，所有技巧围绕的都是神经网络是否能够生成与已知正确的值非常接近的值

人们用不同的名称来表示这一精确度测量者，通常称之为损失或代价

对于本测验，你将使用均方差 (MSE) 计算代价。如下所示：

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnszyk7accj20e50243yf.jpg)

此处，w 表示网络中所有的权重集合，b 表示所有的偏置，m 表示训练示例的总数，a 是 y(x) 的近视值，a 和 y(x) 都是长度相同的向量。

权重集合是所有权重矩阵压平成的向量，串联成一个大的向量。偏置也相似，但是它们已经是向量，所以在串联前不需要压平。

以下是创建 w 的代码示例：

```bash
# 2 by 2 matrices
w1  = np.array([[1, 2], [3, 4]])
w2  = np.array([[5, 6], [7, 8]])

# flatten
w1_flat = np.reshape(w1, -1)
w2_flat = np.reshape(w2, -1)

w = np.concatenate((w1_flat, w2_flat))
# array([1, 2, 3, 4, 5, 6, 7, 8])
```

这样可以轻松地将神经网络中使用的所有权重和偏置提取出来，从而更轻松地编写代码，我们将在接下来的梯度下降部分看到。

注意：你不需要在你的代码中实现！只是将权重和偏置看做集合比单独对待更容易处理

代价 C 取决于正确输出 y(x) 和网络的输出 a 之间的差值。很容易看出 y(x) 和 a (对于 x 的所有值）) 之间的差始终不为 0

这是理想情况，实际上`学习流程就是为了尽量减小代价`
