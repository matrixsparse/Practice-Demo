# 反向传播

## 梯度下降解决方案

```bash
def gradient_descent_update(x, gradx, learning_rate):
    """
    Performs a gradient descent update.
    """
    x = x - learning_rate * gradx
    # Return the new value for x
    return x
```

我们调整了旧的 x，朝着 gradx 的方向推动，推力为 learning_rate。减去 learning_rate * gradx

注意，梯度一开始朝着最陡上升方向，所以将 x 减去 learning_rate * gradx 使其变成最陡下降方向

## 梯度和反向传播

我们现在知道如何使用梯度更新我们的权重和偏置，我们还需要知道如何计算所有节点的梯度。

对于每个节点，我们需要根据梯度更改代价的值（考虑到该节点的值）。这样，我们做出的梯度下降更新最终会实现最低代价。

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsu5m7q2qj20mg0a30tu.jpg)

>用 MiniFlow 编写的话，应该如下所示：

```bash
X, y = Input(), Input()
W1, b1 = Input(), Input()
W2, b2 = Input(), Input()

l1 = Linear(X, W1, b1)
s = Sigmoid(l1)
l2 = Linear(s, W2, b2)
cost = MSE(l2, y)
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsu9f1tm5j20mg04pglt.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsuavedo2j20md07qdge.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsudlgd4sj20ms0atgn7.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsugysuy8j20n10cndh8.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsugytp9vj20m30j3wfx.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsujzd1lxj20np0ajq4i.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsukjqx8fj20mq09w0tk.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsuotvxt1j20hy0aymxu.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsupee3l3j20m204umxk.jpg)

要算出梯度，只需将它前面所有节点（从代价那开始）的梯度相乘。这就是反向传播概念

梯度在网络上向后传播，并使用梯度下降来更新权重和偏置

如果某个节点具有多个向外的节点，则直接将每个节点的梯度相加即可

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsuqe5y9sj20mv044dgd.jpg)

```bash
# Initialize a partial for each of the inbound_nodes.
self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}
# Cycle through the outputs. The gradient will change depending
# on each output, so the gradients are summed over all outputs.
for n in self.outbound_nodes:
    # Get the partial of the cost with respect to this node.
    grad_cost = n.gradients[self]
    # Set the partial of the loss with respect to this node's inputs.
    self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)
    # Set the partial of the loss with respect to this node's weights.
    self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)
    # Set the partial of the loss with respect to this node's bias.
    self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)
```

## 新的代码

自上次查看 MiniFlow 起，MiniFlow 已经出现了几处更改：

首先是 Node 类现在具有一个 backward 方法，并且添加了新的属性 self.gradients，用于在反向传递过程中存储和缓存梯度

```bash
class Node(object):
    """
    Base class for nodes in the network.

    Arguments:

        `inbound_nodes`: A list of nodes with edges into this node.
    """
    def __init__(self, inbound_nodes=[]):
        """
        Node's constructor (runs when the object is instantiated). Sets
        properties that all nodes need.
        """
        # A list of nodes with edges into this node.
        self.inbound_nodes = inbound_nodes
        # The eventual value of this node. Set by running
        # the forward() method.
        self.value = None
        # A list of nodes that this node outputs to.
        self.outbound_nodes = []
        # New property! Keys are the inputs to this node and
        # their values are the partials of this node with
        # respect to that input.
        self.gradients = {}
        # Sets this node as an outbound node for all of
        # this node's inputs.
        for node in inbound_nodes:
            node.outbound_nodes.append(self)

    def forward(self):
        """
        Every node that uses this class as a base class will
        need to define its own `forward` method.
        """
        raise NotImplementedError

    def backward(self):
        """
        Every node that uses this class as a base class will
        need to define its own `backward` method.
        """
        raise NotImplementedError
```

第二项更改是辅助函数 forward_pass()。该函数被替换成了 forward_and_backward()

```bash
def forward_and_backward(graph):
    """
    Performs a forward pass and a backward pass through a list of sorted nodes.

    Arguments:

        `graph`: The result of calling `topological_sort`.
    """
    # Forward pass
    for n in graph:
        n.forward()

    # Backward pass
    # see: https://docs.python.org/2.3/whatsnew/section-slices.html
    for n in graph[::-1]:
        n.backward()
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsuva77c6j20id04nglz.jpg)
