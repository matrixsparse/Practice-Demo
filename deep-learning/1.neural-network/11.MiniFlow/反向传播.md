# 反向传播

## 梯度下降解决方案

```bash
def gradient_descent_update(x, gradx, learning_rate):
    """
    Performs a gradient descent update.
    """
    x = x - learning_rate * gradx
    # Return the new value for x
    return x
```

我们调整了旧的 x，朝着 gradx 的方向推动，推力为 learning_rate。减去 learning_rate * gradx

注意，梯度一开始朝着最陡上升方向，所以将 x 减去 learning_rate * gradx 使其变成最陡下降方向
