# 反向传播

## 梯度下降解决方案

```bash
def gradient_descent_update(x, gradx, learning_rate):
    """
    Performs a gradient descent update.
    """
    x = x - learning_rate * gradx
    # Return the new value for x
    return x
```

我们调整了旧的 x，朝着 gradx 的方向推动，推力为 learning_rate。减去 learning_rate * gradx

注意，梯度一开始朝着最陡上升方向，所以将 x 减去 learning_rate * gradx 使其变成最陡下降方向

## 梯度和反向传播

我们现在知道如何使用梯度更新我们的权重和偏置，我们还需要知道如何计算所有节点的梯度。

对于每个节点，我们需要根据梯度更改代价的值（考虑到该节点的值）。这样，我们做出的梯度下降更新最终会实现最低代价。

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsu5m7q2qj20mg0a30tu.jpg)

>用 MiniFlow 编写的话，应该如下所示：

```bash
X, y = Input(), Input()
W1, b1 = Input(), Input()
W2, b2 = Input(), Input()

l1 = Linear(X, W1, b1)
s = Sigmoid(l1)
l2 = Linear(s, W2, b2)
cost = MSE(l2, y)
```

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsu9f1tm5j20mg04pglt.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsuavedo2j20md07qdge.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsudlgd4sj20ms0atgn7.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsugysuy8j20n10cndh8.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsugytp9vj20m30j3wfx.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsujzd1lxj20np0ajq4i.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsukjqx8fj20mq09w0tk.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsuotvxt1j20hy0aymxu.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsupee3l3j20m204umxk.jpg)

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsuqdy6qvj20m204umxk.jpg)

要算出梯度，只需将它前面所有节点（从代价那开始）的梯度相乘。这就是反向传播概念

梯度在网络上向后传播，并使用梯度下降来更新权重和偏置

如果某个节点具有多个向外的节点，则直接将每个节点的梯度相加即可

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsuqe5y9sj20mv044dgd.jpg)
