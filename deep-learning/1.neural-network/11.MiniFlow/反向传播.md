# 反向传播

## 梯度下降解决方案

```bash
def gradient_descent_update(x, gradx, learning_rate):
    """
    Performs a gradient descent update.
    """
    x = x - learning_rate * gradx
    # Return the new value for x
    return x
```

我们调整了旧的 x，朝着 gradx 的方向推动，推力为 learning_rate。减去 learning_rate * gradx

注意，梯度一开始朝着最陡上升方向，所以将 x 减去 learning_rate * gradx 使其变成最陡下降方向

## 梯度和反向传播

我们现在知道如何使用梯度更新我们的权重和偏置，我们还需要知道如何计算所有节点的梯度。

对于每个节点，我们需要根据梯度更改代价的值（考虑到该节点的值）。这样，我们做出的梯度下降更新最终会实现最低代价。

![All text](http://ww1.sinaimg.cn/large/dc05ba18ly1fnsu5m7q2qj20mg0a30tu.jpg)

>用 MiniFlow 编写的话，应该如下所示：

```bash
X, y = Input(), Input()
W1, b1 = Input(), Input()
W2, b2 = Input(), Input()

l1 = Linear(X, W1, b1)
s = Sigmoid(l1)
l2 = Linear(s, W2, b2)
cost = MSE(l2, y)
```
