# error cause

## 误差原因

在模型预测中，模型可能出现的误差来自两个主要来源，即：

因模型`无法表示基本数据的复杂度`而造成的`偏差（bias）`，

或者因`模型对训练`它所用的`有限数据过度敏感`而造成的`方差（variance）`

## 偏差造成的误差 - 准确率和欠拟合


如前所述，如果模型具有足够的数据，但因`不够复杂`而`无法捕捉基本关系`，则会出现偏差

这样一来，模型一直会系统地`错误表示数据`，从而导致准确率降低,这种现象叫做`欠拟合（underfitting）`

简单来说，如果模型不适当，就会出现偏差。举个例子：如果对象是按颜色和形状分类的，但模型只能按颜色来区分对象和将对象分类（模型过度简化），因而一直会错误地分类对象。

或者，我们可能有本质上是多项式的连续数据，但模型只能表示线性关系

在此情况下，我们向模型提供多少数据并不重要，因为模型根本无法表示其中的基本关系，我们需要更复杂的模型


## 方差造成的误差 - 精度和过拟合


在训练模型时，通常使用来自`较大训练集`的`有限数量样本`

如果利用随机选择的数据子集反复训练模型，可以预料它的预测结果会因提供给它的具体样本而异

方差（variance）用来测量预测结果对于任何给定的测试样本会出现多大的变化

出现方差是正常的，但`方差过高`表明模型无法将其预测结果泛化到更多的数据

对`训练集高度敏感也称为过拟合（overfitting）`，而且通常出现在模型过于复杂或我们没有足够的数据支持它时

通常，可以利用更多数据进行训练，以降低模型预测结果的方差并提高精度

如果没有更多的数据可以用于训练，还可以通过限制模型的复杂度来降低方差

## 如何辨别模型表现的好坏

```bash
from sklearn.learning_curve import learning_curve # sklearn 0.17
from sklearn.model_selection import learning_curve # sklearn 0.18
```

```bash
learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
```

```bash
这里estimator是我们正在用来预测的模型，例如它可以是GaussianNB()，X和y是特征和目标。cv是交叉验证生成器，例如KFold()，'n_jobs'是平行运算的参数，train_sizes是多少数量的训练数据用来生成曲线。
```

## 改进模型的有效性

在给定一组固定数据时，模型不能过于简单或复杂

如果过于简单，模型无法了解数据并会错误地表示数据

如果建立非常复杂的模型，则需要`更多数据`才能`了解基本关系`，否则十分常见的是，模型会推断出在数据中实际上并不存在的关系

关键在于，通过找出正确的模型复杂度来找到最大限度降低偏差和方差的最有效

当然，数据越多，模型随着时间推移会变得越好

## 偏差、方差和特征数量

我们来深入一点观察在算法中使用的特征个数与偏差-方差困境有何关系

高偏差算法就是对训练数据关心很少是一种过度简化

它只是一次又一次地重复做同样的事情，而不管数据可能尝试告诉它要做什么

另一方面，一个太高方差的算法会对数据过度关心，它不会很好地推广至以前未曾见过的新情况

基本上只是记住训练示例而已，一旦获得新的示例或者新的数据点和原有的训练示例不完全一直的时候,就不知道如何去做了

另一种考虑的角度是，它会过度拟合数据

还要说的一件事情就是，高偏差算法倾向于在训练集上有很高误差

所以在进行回归时，可能意味着很低的R平方值或者较大的残差平方和

另一方面，高方差可能对训练集有很好很好的拟合度

但对于测试数据却拟合不佳，因为它不能很好地推广，所以一旦你给它新的输入，立刻就会出现问题

你通常期望在训练上做的测试集好一些，但高方差意味着你在训练集上做的好太多

高方差是当你对训练数据过度拟合后，在测试集上的性能就很差

假设你有一个算法只是用其中的一些特征

比如说你可以使用很多的特征，可能是数十个之中的一两种

